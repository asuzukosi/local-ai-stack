services:
  localai:
    container_name: localai
    hostname: localai
    image: localai/localai:latest-gpu-nvidia-cuda-12
    environment:
      - MODELS_PATH=/models
      - TOKEN=same-token-here
    #  - DEBUG=true
    volumes:
      - ./models:/models:cached
      - ./images/:/tmp/generated/images/
    network_mode: host
    entrypoint:
      - /build/entrypoint.sh
      - worker
      - p2p-llama-cpp-rpc
      - --llama-cpp-args=-m 16380 # set this to the VRAM size in MB
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]